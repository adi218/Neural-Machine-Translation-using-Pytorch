{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from collections import Counter\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torchvision import transforms, utils\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "import pdb\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataHandler:\n",
    "    \n",
    "    \n",
    "    def read_glove_vecs(self, glove_file):\n",
    "        with open(glove_file, 'r') as f:\n",
    "            words = set()\n",
    "            word_to_vec_map = {}\n",
    "            for line in f:\n",
    "                line = line.strip().split()\n",
    "                curr_word = line[0]\n",
    "                words.add(curr_word)\n",
    "                word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float64)\n",
    "\n",
    "            i = 1\n",
    "            words_to_index = {}\n",
    "            index_to_words = {}\n",
    "            for w in sorted(words):\n",
    "                words_to_index[w] = i\n",
    "                index_to_words[i] = w\n",
    "                i = i + 1\n",
    "        return words_to_index, index_to_words, word_to_vec_map\n",
    "\n",
    "\n",
    "    def load_train(self, folder=\"./data/\", rows=-1):\n",
    "        for file in os.listdir(folder):\n",
    "            file_path = os.path.join(os.path.abspath(folder), file)\n",
    "            if file_path.__contains__(\"train\"):\n",
    "                if file_path.endswith(\"en\"):\n",
    "                    file_en = open(file_path)\n",
    "                    dataset_en = self._read_file(file_en)\n",
    "                elif file_path.endswith(\"vi\"):\n",
    "                    file_vi = open(file_path)\n",
    "                    dataset_vi = self._read_file(file_vi)\n",
    "        if rows != -1:\n",
    "            return dataset_en.sample(rows), dataset_vi.sample(rows)\n",
    "        return dataset_en, dataset_vi\n",
    "    \n",
    "    def sentences_to_indices(self, X, word_to_index, max_len):\n",
    "    \n",
    "        m = X.shape[0]                                   # number of training examples\n",
    "\n",
    "        X_indices = np.zeros((m, max_len))\n",
    "\n",
    "        for i in range(m):                               # loop over training examples\n",
    "\n",
    "            sentence_words =X[i].lower().split()\n",
    "\n",
    "            j = 0\n",
    "\n",
    "            for w in sentence_words:\n",
    "                X_indices[i, j] = word_to_index[w]\n",
    "                j = j+1\n",
    "\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "        return X_indices\n",
    "\n",
    "    @staticmethod\n",
    "    def _read_file(file):\n",
    "\n",
    "        lines = file.readlines()\n",
    "        lst_lines = [x.strip() for x in lines]\n",
    "        return pd.DataFrame(lst_lines)\n",
    "    \n",
    "    @staticmethod\n",
    "    def _tokenize(line, normalize_digits=True):\n",
    "        \n",
    "        line = re.sub('<u>', '', line)\n",
    "        line = re.sub('</u>', '', line)\n",
    "        line = re.sub('\\[', '', line)\n",
    "        line = re.sub('\\]', '', line)\n",
    "        words = []\n",
    "        _WORD_SPLIT = re.compile(\"([.,!?\\\"'-<>:;)(])\")\n",
    "        _DIGIT_RE = re.compile(r\"\\d\")\n",
    "        for fragment in line.strip().lower().split():\n",
    "            for token in re.split(_WORD_SPLIT, fragment):\n",
    "                if not token:\n",
    "                    continue\n",
    "                if normalize_digits:\n",
    "                    token = re.sub(_DIGIT_RE, '#', token)\n",
    "                words.append(token)\n",
    "        return words\n",
    "    \n",
    "    @staticmethod\n",
    "    def _encode_seq(sample, vocab):\n",
    "        for i, token in enumerate(sample):\n",
    "            # pdb.set_trace()\n",
    "            if token in self.vocab:\n",
    "\n",
    "                sample[i] = self.vocab[token]\n",
    "            else:\n",
    "                sample[i] = self.vocab['UNK']\n",
    "        return sample\n",
    "    \n",
    "    def _pad(sample, maxlen):\n",
    "        if len(sample) > self.maxlen:\n",
    "            return sample[:self.maxlen]\n",
    "        pad_required = (self.maxlen - len(sample))*[0]\n",
    "        return sample + pad_required\n",
    "    \n",
    "    def _to_tensor(sample):\n",
    "        return torch.LongTensor(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "class LanguageDataset(Dataset):\n",
    "    def __init__(self, lang_file, root_dir, transform=None):\n",
    "        with open(os.path.join(root_dir, lang_file), 'r') as f:\n",
    "            self.sequences = f.readlines()[:50000]\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.transform(self.sequences[idx]) if self.transform else self.sequences[idx]\n",
    "\n",
    "class Tokenize(object):\n",
    "    def compose(self, func1, func2):\n",
    "        return lambda x:func1(func2(x))\n",
    "    def __call__(self, sample):\n",
    "        lower_and_strip = self.compose(str.rstrip, str.lower)\n",
    "        sample = lower_and_strip(sample)\n",
    "        return sample.split()\n",
    "\n",
    "class EncodeSeq(object):\n",
    "    def __init__(self, vocab):\n",
    "        self.vocab = vocab\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        for i, token in enumerate(sample):\n",
    "            # pdb.set_trace()\n",
    "            if token in self.vocab:\n",
    "                \n",
    "                sample[i] = self.vocab[token]\n",
    "            else:\n",
    "                sample[i] = self.vocab['UNK']\n",
    "        return sample\n",
    "\n",
    "class Pad(object):\n",
    "    def __init__(self, maxlen):\n",
    "        self.maxlen = maxlen\n",
    "    def __call__(self, sample):\n",
    "        if len(sample) > self.maxlen:\n",
    "            return sample[:self.maxlen]\n",
    "        pad_required = (self.maxlen - len(sample))*[0]\n",
    "        return sample + pad_required\n",
    "class ToTensor(object):\n",
    "    def __call__(self, sample):\n",
    "        return torch.LongTensor(sample)\n",
    "\n",
    "\n",
    "def train(train_x):\n",
    "    for i_batch, sample_batched in enumerate(train_x):\n",
    "        x = sample_batched\n",
    "        print(x.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Lang():\n",
    "    def __init__(self, sequences, n_words):\n",
    "        self.sequences = sequences\n",
    "        self.n_words = n_words\n",
    "        super(Lang, self).__init__()\n",
    "    def compose(self, func1, func2):\n",
    "        return lambda x:func1(func2(x))\n",
    "    def tokenize(self, sequence):\n",
    "        lower_and_strip = self.compose(str.rstrip, str.lower)\n",
    "        sample = lower_and_strip(sequence)\n",
    "        return sample.split()\n",
    "    def build_vocab(self):\n",
    "        self.all = []\n",
    "        counter = Counter()\n",
    "        def pool(tokens):\n",
    "            self.all.extend(tokens)\n",
    "        tokenize_and_pool = self.compose(pool, self.tokenize)\n",
    "        for i in range(len(self.sequences)):\n",
    "            tokenize_and_pool(self.sequences[i])\n",
    "        return [word for word, _ in Counter(self.all).most_common(self.n_words - 3)]\n",
    "        # return list(set(self.all))\n",
    "    def word2index(self):\n",
    "        index={}\n",
    "        self.vocab = self.build_vocab()\n",
    "        self.vocab.insert(0, 'zero'); self.vocab.extend(['UNK','$'])\n",
    "        for word in self.vocab:\n",
    "            index[word] = len(index)\n",
    "        return index\n",
    "    def index2word(self):\n",
    "        self.index = self.word2index()\n",
    "        return {k:i for i, k in self.index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRnn(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size, dropout, use_cuda):\n",
    "        super(EncoderRnn, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.use_cuda = use_cuda\n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
    "#         self.embedding.weight.data.copy_(torch.from_numpy(emb_matrix))\n",
    "        self.lstm = nn.LSTMCell(hidden_size, hidden_size)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "    def forward(self, inputs):\n",
    "        embedded = self.drop(self.embedding(inputs))\n",
    "        h_t = Variable(torch.zeros(inputs.size(0), self.hidden_size))\n",
    "        c_t = Variable(torch.zeros(inputs.size(0), self.hidden_size))\n",
    "        if self.use_cuda:\n",
    "            h_t = h_t.cuda()\n",
    "            c_t = c_t.cuda()\n",
    "        for i, input_t in enumerate(embedded.chunk(embedded.size(1), dim=1)):\n",
    "            h_t, c_t = self.lstm(torch.squeeze(input_t), (h_t, c_t))\n",
    "        return h_t\n",
    "\n",
    "class DecoderRnn(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size, dropout, use_cuda):\n",
    "        super(DecoderRnn, self).__init__()                  \n",
    "        self.hidden_size = hidden_size\n",
    "        self.use_cuda = use_cuda\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.lstm = nn.LSTMCell(hidden_size, hidden_size)\n",
    "        self.lstm2 = nn.LSTMCell(hidden_size, hidden_size)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "    def forward(self, inputs, hiddens, target_sequences, use_tf):\n",
    "        embedded = self.drop(self.embedding(inputs))\n",
    "        target_len = target_sequences.size(1)\n",
    "        output= []\n",
    "        h_t = hiddens\n",
    "        c_t = Variable(torch.zeros(inputs.size(0), self.hidden_size))\n",
    "        h_t2 = Variable(torch.zeros(inputs.size(0), self.hidden_size))\n",
    "        c_t2 = Variable(torch.zeros(inputs.size(0), self.hidden_size))\n",
    "        if self.use_cuda:\n",
    "            c_t = c_t.cuda()\n",
    "            h_t2 = h_t2.cuda()\n",
    "            c_t2 = c_t2.cuda() \t\n",
    "        batch_size = embedded.size(1)\n",
    "        for i in range(target_len):\n",
    "            h_t, c_t = self.lstm(torch.squeeze(embedded), (h_t, c_t))\n",
    "            h_t2, c_t2 = self.lstm2(h_t, (h_t2, c_t2))\n",
    "            if use_tf:\n",
    "                embedded = self.drop(self.embedding(target_sequences.permute(1,0)[i]))\n",
    "            else:\n",
    "                out = F.softmax(self.linear(h_t2)).max(1)[1]\n",
    "                embedded = self.drop(self.embedding(out))\n",
    "            output.append(F.softmax(self.linear(h_t2)))\n",
    "        output = torch.stack(output)\n",
    "        return output, h_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def pretrained_embedding_layer(word_to_vec_map, word_to_index):\n",
    "     \n",
    "#     vocab_len = len(word_to_index) + 1                  \n",
    "#     emb_dim = word_to_vec_map[\"cucumber\"].shape[0]      \n",
    "    \n",
    "#     emb_matrix = np.zeros((vocab_len, emb_dim))\n",
    "    \n",
    "#     for word, index in word_to_index.items():\n",
    "#         emb_matrix[index, :] = word_to_vec_map[word]\n",
    "    \n",
    "#     embedding_layer = nn.Embedding(vocab_len, emb_dim)\n",
    "    \n",
    "#     embedding_layer.weight.data.copy_(torch.from_numpy(emb_matrix))\n",
    "    \n",
    "    \n",
    "#     return embedding_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29491\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "teacher_forcing_ratio=0.5\n",
    "def train(batch_input_sequences, batch_target_sequences, \n",
    "    criterion, encoder, decoder, lr,  eos_tok, use_cuda):\n",
    "    encoder_optimizer = torch.optim.Adam(encoder.parameters(), lr=lr)\n",
    "    decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=lr)\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    use_tf = True if random.random() < teacher_forcing_ratio else False\n",
    "    if use_cuda:\n",
    "        batch_input_sequences = (batch_input_sequences).cuda()\n",
    "        batch_target_sequences = (batch_target_sequences).cuda()\n",
    "    batch_input_sequences = Variable(batch_input_sequences)\n",
    "    batch_target_sequences = Variable(batch_target_sequences)\n",
    "    encoder.train(); decoder.train()\n",
    "    encoder_hiddens = encoder(batch_input_sequences)\n",
    "    decoder_inputs = np.zeros((len(batch_input_sequences), 1))\n",
    "    for i in range(len(batch_input_sequences)):\n",
    "        decoder_inputs[i] = eos_tok\n",
    "    decoder_inputs = torch.from_numpy(decoder_inputs).long()\n",
    "    if use_cuda:\n",
    "        decoder_inputs = decoder_inputs.cuda()\n",
    "    decoder_inputs = Variable(decoder_inputs)\n",
    "    decoder_hiddens = encoder_hiddens\n",
    "            \n",
    "    decoder_outputs, decoder_hiddens= decoder(decoder_inputs, decoder_hiddens, batch_target_sequences, use_tf)\n",
    "    loss = 0\n",
    "    for i, d_o in enumerate(decoder_outputs):\n",
    "        loss += criterion(d_o, batch_target_sequences.permute(1, 0)[i])\n",
    "    loss.backward()\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "    return loss.data[0]\n",
    "        \n",
    "def validate(batch_input_sequences, batch_target_sequences, \n",
    "    criterion, encoder, decoder, lr,  eos_tok, use_cuda):\n",
    "    if use_cuda:\n",
    "        batch_input_sequences = (batch_input_sequences).cuda()\n",
    "        batch_target_sequences = (batch_target_sequences).cuda()\n",
    "    batch_input_sequences = Variable(batch_input_sequences)\n",
    "    batch_target_sequences = Variable(batch_target_sequences)\n",
    "    encoder.eval(); decoder.eval()\n",
    "    encoder_hiddens = encoder(batch_input_sequences)\n",
    "    decoder_inputs = np.zeros((len(batch_input_sequences), 1))\n",
    "    for i in range(len(batch_input_sequences)):\n",
    "        decoder_inputs[i] = eos_tok\n",
    "    decoder_inputs = torch.from_numpy(decoder_inputs).long()\n",
    "    if use_cuda:\n",
    "        decoder_inputs = decoder_inputs.cuda()\n",
    "    decoder_inputs = Variable(decoder_inputs)\n",
    "    decoder_hiddens = encoder_hiddens\n",
    "            \n",
    "    decoder_outputs, decoder_hiddens= decoder(decoder_inputs, decoder_hiddens, batch_target_sequences, 0)\n",
    "    loss = 0\n",
    "    # pdb.set_trace()\n",
    "    for i, d_o in enumerate(decoder_outputs):\n",
    "        loss += criterion(d_o, batch_target_sequences.permute(1, 0)[i])\n",
    "    return loss.data[0]\n",
    "\n",
    "\n",
    "# ap = argparse.ArgumentParser()\n",
    "# ap.add_argument('-max_len', type=int, default=100)\n",
    "# ap.add_argument('-vocab_size', type=int, default=100)\n",
    "# ap.add_argument('-batch_size', type=int, default=128)\n",
    "# ap.add_argument('-hidden_dim', type=int, default=300)\n",
    "# ap.add_argument('-dropout', type=float, default=0.3)\n",
    "# ap.add_argument('-nb_epoch', type=int, default=25)\n",
    "# ap.add_argument('-learning_rate', type=int, default=0.01)\n",
    "# ap.add_argument('-log_step', type=int, default=5)\n",
    "# args = vars(ap.parse_args())\n",
    "\n",
    "MAX_LEN = 100\n",
    "VOCAB_SIZE = 50000\n",
    "BATCH_SIZE =8\n",
    "HIDDEN_DIM = 300\n",
    "NB_EPOCH = 25\n",
    "LEARNING_RATE = 0.001\n",
    "DROPOUT = 0.3\n",
    "dh = DataHandler()\n",
    "def save(model_ft, filename):\n",
    "    save_filename = filename\n",
    "    torch.save(model_ft, save_filename)\n",
    "    print('Saved as %s' % save_filename)\n",
    "# creating vocabulary\n",
    "def create_vocab(filename):\n",
    "    sentences = LanguageDataset(lang_file=filename, root_dir='data')\n",
    "    lang_obj = Lang(sentences, VOCAB_SIZE)\n",
    "    return lang_obj.word2index()\n",
    "# pair wise matching\n",
    "def pairwise(input_sequences, target_sequences, indices):\n",
    "    inp = torch.stack([input_sequences[indices[i]] for i in range(len(indices))])\n",
    "    tar = torch.stack([target_sequences[indices[i]] for i in range(len(indices))])\n",
    "    return inp, tar\n",
    "\n",
    "\n",
    "vocab_eng = create_vocab('train.en')\n",
    "print(len(vocab_eng))\n",
    "vocab_vi = create_vocab('train.vi')\n",
    "\n",
    "# words_to_index, index_to_words, word_to_vec_map = dh.read_glove_vecs('data/glove.6B.50d.txt')\n",
    "\n",
    "\n",
    "#Loading Data\n",
    "input_sequences = LanguageDataset(lang_file='train.en', root_dir='data',\n",
    "                                transform=transforms.Compose([Tokenize(),EncodeSeq(vocab_eng),\n",
    "                                                                Pad(MAX_LEN),\n",
    "                                                                ToTensor()]))\n",
    "target_sequences = LanguageDataset(lang_file='train.vi', root_dir='data',\n",
    "                                transform=transforms.Compose([Tokenize(),EncodeSeq(vocab_vi),\n",
    "                                                                Pad(MAX_LEN),\n",
    "                                                                ToTensor()]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading encoder and decoder models\n",
      "8\n",
      "40000\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "index out of range at /pytorch/aten/src/TH/generic/THTensorEvenMoreMath.cpp:191",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-965ab9251b78>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m             train_loss = train(train_batch_input_sequences, train_batch_target_sequences, \n\u001b[1;32m     36\u001b[0m                         \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m                         LEARNING_RATE, eos_tok, use_gpu)\n\u001b[0m\u001b[1;32m     38\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0mval_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-f366ad539b08>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(batch_input_sequences, batch_target_sequences, criterion, encoder, decoder, lr, eos_tok, use_cuda)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mdecoder_hiddens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_hiddens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mdecoder_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_hiddens\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_hiddens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_target_sequences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_tf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_o\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-ecb7110b1ba4>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs, hiddens, target_sequences, use_tf)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhiddens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_sequences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_tf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0membedded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0mtarget_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_sequences\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    116\u001b[0m         return F.embedding(\n\u001b[1;32m    117\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   1452\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1453\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1454\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: index out of range at /pytorch/aten/src/TH/generic/THTensorEvenMoreMath.cpp:191"
     ]
    }
   ],
   "source": [
    "indices = list(range(len(input_sequences)))\n",
    "split_index = int(0.8*len(indices))\n",
    "np.random.shuffle(indices)\n",
    "train_idx, valid_idx = indices[:split_index], indices[split_index:]\n",
    "\n",
    "\n",
    "train_input_sequences, train_target_sequences = pairwise(input_sequences, target_sequences, train_idx)\n",
    "val_input_sequences, val_target_sequences = pairwise(input_sequences, target_sequences, valid_idx)\n",
    "train_size = len(train_idx)\n",
    "val_size = len(valid_idx)\n",
    "# pdb.set_trace()\n",
    "eos_tok = vocab_vi['$']\n",
    "use_gpu = torch.cuda.is_available()\n",
    "print(\"loading encoder and decoder models\")\n",
    "use_gpu = 0\n",
    "encoder = EncoderRnn(len(vocab_eng), HIDDEN_DIM, DROPOUT, use_gpu)\n",
    "decoder = DecoderRnn(len(vocab_vi), HIDDEN_DIM, DROPOUT, use_gpu)\n",
    "if use_gpu:\n",
    "    print(\"using GPU\")\n",
    "    encoder = encoder.cuda()\n",
    "    decoder = decoder.cuda()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "total_step = (train_size/BATCH_SIZE) +1\n",
    "\n",
    "for epoch in  range(NB_EPOCH):\n",
    "    step = 0\n",
    "    try:\n",
    "        for i in range(0, train_size, BATCH_SIZE):\n",
    "            step+=1\n",
    "            num_samples = min(BATCH_SIZE, train_size - i)\n",
    "            print(num_samples)\n",
    "            print(len(train_input_sequences))\n",
    "            train_batch_input_sequences = train_input_sequences[i:i+num_samples]\n",
    "            train_batch_target_sequences = train_target_sequences[i:i+num_samples]\n",
    "            train_loss = train(train_batch_input_sequences, train_batch_target_sequences, \n",
    "                        criterion, encoder, decoder,\n",
    "                        LEARNING_RATE, eos_tok, use_gpu)\n",
    "            print(train_loss)\n",
    "            val_loss=0.0; count = 0\n",
    "            for j in range(0, val_size, BATCH_SIZE):\n",
    "                count+=1\t\n",
    "                num_samples = min(BATCH_SIZE, val_size - j)\n",
    "                val_batch_input_sequences = val_input_sequences[j:j+num_samples]\n",
    "                val_batch_target_sequences = val_target_sequences[j:j+num_samples]\n",
    "                val_loss += validate(val_batch_input_sequences, val_batch_target_sequences, \n",
    "                            criterion, encoder, decoder,\n",
    "                            LEARNING_RATE, eos_tok, use_gpu)\n",
    "            if step % args['log_step']:\n",
    "                print('Epoch [%d/%d], Step [%d/%d], Train Loss: %.4f \\n'%(epoch+1, NB_EPOCH, step, \n",
    "                total_step, train_loss))\n",
    "                print('Validation Loss %.4f'%(val_loss/float(count)))\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Saving before quit...\")\n",
    "        save(encoder, 'encoder_[%d/%d].pkl'%(step, total_step))\n",
    "        save(decoder, 'decoder_[%d/%d].pkl'%(step, total_step))\n",
    "save(encoder, 'encoder.pkl')\n",
    "save(decoder, 'decoder.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
